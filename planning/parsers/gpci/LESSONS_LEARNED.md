# GPCI Parser Implementation: Lessons Learned

**Date:** 2025-10-17  
**Context:** Implementing GPCI parser from 60% to near-100% test pass rate

---

## üéì **Key Lessons for Future Parser PRDs**

### **1. Test Fixtures Must Match Real CMS Data Format**

**What Happened:**
- CSV fixtures had 2 header rows (title + empty + headers) that weren't documented
- Parser defaulted to reading from row 1, causing column misalignment
- Resulted in `ValueError: could not convert string to float: '025 M'` (MAC data in GPCI column)

**PRD Improvement:**
```markdown
## Test Fixtures Requirements
- Document exact CMS file structure (header rows, footers, metadata lines)
- Include representative samples showing:
  - Header row count and format
  - Data start line
  - Empty rows or separators
- Add fixture provenance section with source file details
```

**Impact:** Prevents 50%+ of debugging time on format mismatches

---

### **2. Alias Maps Need Comprehensive CMS Header Variations**

**What Happened:**
- CMS uses different header formats across years: `"PW GPCI"` vs `"2025 PW GPCI (with 1.0 Floor)"`
- Alias map only had generic variations, not year-specific ones
- Column mapping failed silently, producing invalid data

**PRD Improvement:**
```markdown
## Header Normalization Strategy
- Document all known CMS header variations per dataset
- Include historical header formats (e.g., 2023, 2024, 2025)
- Test against multiple year fixtures to validate alias map
- Add validation warnings for unmapped columns
```

**Impact:** Reduces parser brittleness across CMS data updates

---

### **3. Type Handling Must Be Defensive**

**What Happened:**
- `Decimal(str(x))` failed on integer strings like `"1"` (needs `"1.000"`)
- CSV had integers (`1`, `1.5`) while TXT had formatted decimals (`1.000`, `1.500`)
- Required casting through `float()` first for robust parsing

**PRD Improvement:**
```markdown
## Data Type Casting Requirements
- Document expected input formats AND common variations
- Specify handling for:
  - Empty strings ‚Üí NaN
  - Integer strings ‚Üí Decimal
  - Scientific notation ‚Üí Decimal
  - Whitespace/special characters
- Add explicit error handling with actionable messages
- Test with multiple format variations (CSV, XLSX, TXT)
```

**Impact:** Prevents production failures on format variations

---

### **4. Format Detection Should Be Content-Based, Not Extension-Based**

**What Happened:**
- ZIP file contained `GPCI2025_sample.txt` but was parsed as CSV
- Extension-only detection (`.txt` ‚Üí "assume CSV") failed
- Required regex pattern matching (`r'^\d{5}'` for MAC code) to identify fixed-width

**PRD Improvement:**
```markdown
## Format Detection Strategy
1. **Extension check** (fast path): .xlsx ‚Üí Excel, .csv ‚Üí CSV
2. **Content sniffing** (fallback): Check first 500 bytes for:
   - Fixed-width patterns (e.g., MAC code `^\d{5}`)
   - Delimiter detection (commas, tabs, pipes)
   - Layout registry lookup
3. **Explicit fallback order**: Fixed-width ‚Üí CSV ‚Üí Error

Document detection heuristics and edge cases.
```

**Impact:** Handles ambiguous file formats correctly

---

### **5. Error Messages Should Be Actionable**

**What Happened:**
- Generic errors like `decimal.InvalidOperation` didn't indicate root cause
- No guidance on fixing (e.g., "Check header rows" or "Verify column mapping")
- Required deep debugging to identify issues

**PRD Improvement:**
```markdown
## Error Handling Standards
- Every error must include:
  1. **What failed:** "Failed to convert 'gpci_work' column"
  2. **Why it failed:** "Value '025 M' is not numeric"
  3. **How to fix:** "Check column mapping or header row skipping"
  4. **Context:** Filename, row number, column name
- Add validation checkpoints with clear messages
- Log intermediate states for debugging
```

**Impact:** Reduces debugging time by 70%+

---

### **6. Pre-Implementation Validation is Critical**

**What Happened:**
- Assumed CSV format matched TXT format
- Didn't verify header structure before implementation
- Cost ~2 hours of debugging after implementation

**PRD Improvement:**
```markdown
## Pre-Implementation Checklist
Required before coding:
1. ‚úÖ Inspect all source file formats (TXT, CSV, XLSX, ZIP)
2. ‚úÖ Document header structure for each format
3. ‚úÖ Verify column positions/names match across formats
4. ‚úÖ Test layout registry against real data
5. ‚úÖ Validate sample parsing with pandas before implementation
6. ‚úÖ Document CMS-specific quirks (floors, special values, etc.)

Add "Format Verification" step to all parser PRDs.
```

**Impact:** Prevents rework and ensures correct first implementation

---

### **7. CMS Data Quirks Need Explicit Documentation**

**What Happened:**
- CMS GPCI files have 1.0 work GPCI floor (not in raw data)
- Alaska has 1.5 floor (documented in regulations, not files)
- Multiple header rows in CSV/XLSX (not in TXT)
- MAC codes vs Locality codes (different meanings)

**PRD Improvement:**
```markdown
## CMS Dataset Characteristics Section
Required for each dataset PRD:

### File Format Variations
- TXT: Fixed-width, no headers, data starts line X
- CSV: 2 header rows (title + empty), then column headers
- XLSX: Similar to CSV, may have duplicate header rows

### Business Rules (Non-Obvious)
- GPCI work floor: 1.0 (all localities)
- Alaska exception: 1.5 floor (locality 01)
- MAC codes: 5 digits, not same as locality codes
- Effective dates: Derived from quarter (A=Jan, B=Apr, C=Jul, D=Oct)

### Known Data Issues
- Duplicate locality 00 (Alabama + Arizona in some releases)
- Occasional missing columns (handle gracefully)
- Whitespace variations in headers
```

**Impact:** Reduces domain knowledge gaps, improves parser robustness

---

### **8. Test Coverage Should Include Format-Specific Edge Cases**

**What Happened:**
- TXT tests passed, CSV/XLSX tests failed
- Didn't test empty string handling until production
- Metrics calculation failed on empty GPCI values

**PRD Improvement:**
```markdown
## Test Strategy Requirements
Per format (TXT, CSV, XLSX, ZIP):
1. **Golden tests:** Valid data, all columns present
2. **Edge cases:**
   - Empty strings in numeric columns
   - Missing optional columns
   - Duplicate keys
   - Out-of-range values
3. **Format-specific:**
   - CSV: Multiple header rows, quoted fields
   - XLSX: Excel number formatting, date coercion
   - ZIP: Multiple inner files, nested ZIPs
   - TXT: Fixed-width misalignment, truncated lines

Require 80%+ test coverage before PR approval.
```

**Impact:** Catches format-specific bugs before production

---

### **9. Metrics Calculation Should Handle Missing/Empty Data**

**What Happened:**
- `float(df['gpci_work'].min())` failed when column had empty strings
- No null checks before type conversion
- Required filtering: `df[df['gpci_work'] != '']`

**PRD Improvement:**
```markdown
## Metrics Calculation Standards
- Always filter null/empty values before aggregation:
  ```python
  valid_values = df[df['column'] != '']['column']
  min_val = float(valid_values.min()) if len(valid_values) > 0 else None
  ```
- Document expected metrics and their null handling
- Add validation for metric sanity (e.g., GPCI range 0.5-2.0)
- Log warnings for unexpected metric values
```

**Impact:** Prevents runtime errors on edge case data

---

### **10. Incremental Testing is More Efficient Than Big-Bang Testing**

**What Happened:**
- Implemented full parser (510 lines) before running any tests
- Hit 4 different issues simultaneously
- Would have been faster to test CSV parsing alone first

**PRD Improvement:**
```markdown
## Implementation Phasing Strategy
Recommend incremental implementation with testing at each phase:

**Phase 1:** Single format (TXT) + core logic
- Parse fixed-width
- Column normalization
- Type casting
- **Test checkpoint:** 100% TXT tests passing

**Phase 2:** Add CSV/XLSX support
- Extend format detection
- Add header skipping
- Update alias map
- **Test checkpoint:** 100% CSV/XLSX tests passing

**Phase 3:** Add ZIP support
- Inner file extraction
- Format detection on inner file
- **Test checkpoint:** 100% ZIP tests passing

**Phase 4:** Edge cases + integration
- Negative tests
- Payment spot-check
- **Test checkpoint:** 100% all tests passing

Benefits: Faster debugging, clearer error isolation, incremental progress
```

**Impact:** Reduces overall implementation + debugging time by 40%

---

## üìã **Action Items for PRD Template Updates**

1. **Add "Format Verification" section** to all parser PRDs
2. **Create "CMS Dataset Characteristics" template** for domain knowledge
3. **Expand "Error Handling" section** with actionable message requirements
4. **Add "Test Coverage Matrix"** showing format √ó edge case grid
5. **Include "Pre-Implementation Checklist"** in parser standards
6. **Document "Incremental Implementation"** best practice
7. **Add "Metrics Calculation Standards"** to parser contracts
8. **Create "Common CMS Quirks"** reference document

---

## üéØ **Success Metrics**

Track these improvements in future parser implementations:
- **Time to first working test:** Target < 2 hours
- **Debugging time after implementation:** Target < 1 hour
- **Test pass rate on first run:** Target > 80%
- **PRD completeness score:** Target 100% (all sections filled)
- **Format variation handling:** Target 100% (all formats work)

---

**Overall Impact:** These improvements should reduce total parser implementation + debugging time from ~8 hours to ~4 hours.

---

## üéì **NEW LESSONS: QTS Compliance & Testing Philosophy (2025-10-17)**

### **11. Golden Fixtures Must Be Clean, Edge Cases Tested Separately**

**What Happened:**
- Initial fixtures had duplicate locality 00 (Alabama + Arizona from real CMS data)
- Tests expected rejects (`assert len(result.rejects) == 2`)
- Violated STD-qa-testing-prd ¬ß5.1: "Validate goldens against schema contracts"
- CF parser pattern: `assert len(result.rejects) == 0` for clean fixtures

**Root Cause:**
- Confusion between "test real data" vs "test with clean data"
- Real CMS files DO have duplicate locality 00 (authentic quirk)
- But golden tests should validate happy path, not quirks

**PRD Improvement:**
```markdown
## Golden Test Fixture Requirements (¬ß5.1 Enhancement)

**Principle:** Golden fixtures test the happy path with clean, idealized data.

**Requirements:**
1. **Zero Expected Rejects:** Golden tests must assert `len(result.rejects) == 0`
2. **Clean Data:** Remove duplicates, fix out-of-range values, fill missing columns
3. **Identical Across Formats:** TXT/CSV/XLSX/ZIP must have identical data for true format consistency validation
4. **Deterministic:** Use exact assertions (`== 18`) not flexible (`>= 18`)

**Real-World Quirks:** Test separately in edge case fixtures
- Create `tests/fixtures/<dataset>/edge_cases/` directory
- Document authentic CMS data issues (duplicate codes, unusual values)
- Test error handling with real data patterns
- Assert `len(result.rejects) > 0` for known quirks

**Example:**
```python
# Golden test (clean data)
@pytest.mark.golden
def test_gpci_golden_txt():
    # Clean fixture: 18 unique localities (removed duplicate locality 00)
    assert len(result.data) == 18
    assert len(result.rejects) == 0  # ‚Üê Clean fixtures have no rejects

# Edge case test (real CMS quirk)
@pytest.mark.edge_case  
def test_gpci_duplicate_locality_00():
    # Real CMS data: AL and AZ both use locality 00
    assert len(result.rejects) == 2  # ‚Üê Quirk produces expected rejects
    assert 'duplicate' in rejects_msg
```

**Benefits:**
- Golden tests establish clean baseline
- Edge tests validate production error handling
- Standards compliance (QTS ¬ß5.1)
- Clear separation of concerns
```

**Impact:** Prevents QTS compliance violations, establishes proper test organization

---

### **12. Test-Only Flags/Modes Violate Production Parity**

**What Happened:**
- Added `skip_row_count_validation: True` flag to test metadata
- Tests exercised different code path than production
- Could miss bugs in validation logic that only appear in production
- No equivalent in CF parser (production parity maintained)

**Root Cause:**
- Fixtures had too few rows for production threshold (18 vs 100-120)
- Instead of adjusting threshold tiers, bypassed validation
- Created test-vs-production divergence

**PRD Improvement:**
```markdown
## Test-Production Parity Requirements

**Prohibition:** No test-only flags or special modes that change parser behavior.

**Violations:**
- ‚ùå `skip_validation: True` flags
- ‚ùå `test_mode: True` parameters
- ‚ùå Environment checks (`if ENV == 'test'`)
- ‚ùå Different code paths for tests vs production

**Correct Approach:** Adjust validation thresholds to support both test and production data
```python
# ‚ùå BAD: Bypass validation in tests
if not metadata.get('skip_row_count_validation', False):
    validate_row_count(df)

# ‚úÖ GOOD: Tiered thresholds support both
def validate_row_count(df):
    count = len(df)
    if count == 0:
        raise ParseError("Empty file")  # Fail always
    elif 1 <= count < 10:
        logger.warning("INFO: Edge case fixture")  # Warn for small
    elif 10 <= count < 100:
        logger.warning("INFO: Test fixture")  # Warn for medium
    elif count < 100 or count > 120:
        logger.warning("WARNING: Production threshold")  # Warn for prod
```

**Testing Strategy:**
- Tests use exact same code path as production
- Validation levels: FAIL (critical) vs WARN (informational)
- Edge cases can produce warnings without failing tests
- Production validation preserved
```

**Impact:** Ensures tests validate actual production behavior

---

### **13. Exact Assertions Enable True Regression Detection**

**What Happened:**
- Used flexible assertions: `assert len(result.data) >= 18`
- Allowed test to pass with 18, 19, 20, 100+ rows
- Couldn't detect if parser suddenly returned too many/few rows
- CF parser uses exact: `assert len(result.data) == 2`

**Root Cause:**
- Uncertainty about exact row counts across formats
- Tried to make tests "flexible" to avoid brittleness
- Actually made tests less effective at catching regressions

**PRD Improvement:**
```markdown
## Assertion Precision Requirements

**Principle:** Exact assertions detect regressions; flexible assertions hide problems.

**Requirements:**
1. **Exact Counts:** Use `==` for row counts, reject counts, metrics
   ```python
   # ‚ùå BAD: Could miss regressions
   assert len(result.data) >= 18  # Passes with 18, 100, 1000...
   
   # ‚úÖ GOOD: Detects any deviation
   assert len(result.data) == 18  # Only passes with exactly 18
   ```

2. **Exact Values:** Use `==` for GPCI values, dates, strings
   ```python
   # ‚ùå BAD
   assert alaska['gpci_work'] > 1.0
   
   # ‚úÖ GOOD
   assert alaska['gpci_work'] == '1.500'  # Exact match
   ```

3. **When to Use Ranges:** Only for truly variable values
   - ‚úÖ Performance tests: `assert duration < 10.0` (SLO)
   - ‚úÖ Load tests: `assert throughput >= 1000` (capacity)
   - ‚ùå Data tests: Should be exact

**Benefits:**
- Detects unexpected row count changes (parsing bugs)
- Catches precision drift (rounding changes)
- Identifies format inconsistencies
- True regression detection
```

**Impact:** Catches subtle regressions that flexible assertions would miss

---

### **14. Format Fixture Parity is Critical for Consistency Tests**

**What Happened:**
- TXT had 20 rows, CSV had 18 rows, XLSX had 113 rows
- `test_gpci_txt_csv_consistency` had to use "overlapping" logic
- Couldn't verify true format consistency (are outputs identical?)
- Discovered this violated QTS requirement for identical golden data

**Root Cause:**
- Created test fixtures incrementally without coordinating
- Didn't establish "single source of truth" for golden data
- Each format had different sample extraction

**PRD Improvement:**
```markdown
## Multi-Format Fixture Requirements

**Principle:** All format variations (TXT/CSV/XLSX/ZIP) must contain identical data.

**Creation Process:**
1. **Create Master Dataset:** Start with one authoritative fixture (e.g., TXT)
   - Extract N clean rows from CMS data
   - Remove duplicates, fix issues
   - Document exact row count

2. **Derive Other Formats:** Convert master to other formats
   ```bash
   # Master
   head -21 GPCI2025.txt > GPCI2025_sample.txt  # 18 data rows
   
   # Derive CSV (same 18 rows, just convert format)
   python convert_txt_to_csv.py GPCI2025_sample.txt > GPCI2025_sample.csv
   
   # Derive XLSX (same 18 rows)
   python convert_txt_to_xlsx.py GPCI2025_sample.txt > GPCI2025_sample.xlsx
   
   # Derive ZIP (contains master TXT)
   zip GPCI2025_sample.zip GPCI2025_sample.txt
   ```

3. **Verify Parity:** All formats must produce identical parsed output
   ```python
   # True format consistency test
   assert txt_result.data.equals(csv_result.data)
   assert csv_result.data.equals(xlsx_result.data)
   # NOT: assert overlapping_localities >= 10
   ```

4. **Document in README:** State explicitly that all formats are identical

**Exception:** Full dataset XLSX
- If XLSX contains full dataset for comprehensive testing, mark it separately
- Document: "XLSX contains full 100+ row dataset, not sample"
- Create separate test: `test_gpci_golden_xlsx_full_dataset()`
```

**Impact:** Enables true format consistency validation, prevents fixture drift

---

### **15. Tiered Validation Thresholds Support Both Test and Production**

**What Happened:**
- Production validation: `if count < 90: raise ParseError`
- Test fixtures: 18 rows (below threshold)
- Had to add `skip_row_count_validation` flag (bad)
- CF parser doesn't need this because it uses tiered warnings

**Root Cause:**
- Binary validation (fail vs pass) doesn't support test fixtures
- Threshold too high for any test fixture
- Didn't consider INFO/WARN/ERROR severity levels

**PRD Improvement:**
```markdown
## Tiered Validation Threshold Pattern

**Principle:** Use severity levels (INFO/WARN/ERROR) instead of binary pass/fail.

**Pattern:**
```python
def validate_row_count(df: pd.DataFrame) -> Optional[str]:
    """
    Tiered validation supports both test fixtures and production data.
    
    - FAIL: 0 rows (critical)
    - INFO: 1-10 rows (edge case fixtures)
    - INFO: 10-100 rows (test fixtures)
    - WARN: 100-120 rows (production, but check for issues)
    - WARN: >120 rows (unexpected, investigate)
    """
    count = len(df)
    
    # FAIL tier: Critical issues
    if count == 0:
        raise ParseError("Empty file")
    
    # INFO tier: Expected for test data
    if 1 <= count < 10:
        return f"INFO: Minimal fixture ({count} rows). Production expects 100-120."
    if 10 <= count < 100:
        return f"INFO: Test fixture ({count} rows). Production expects 100-120."
    
    # WARN tier: Production boundary issues  
    if count > 120:
        return f"WARN: High count ({count} > 120). Check for duplicates/multi-quarter."
    
    # SUCCESS tier: Production normal range
    return None  # 100-120 is expected, no message
```

**Benefits:**
- No test-only flags needed
- Tests exercise production code path
- Clear severity guidance (INFO = expected, WARN = investigate, ERROR = fail)
- Supports edge cases (3 rows), test fixtures (18 rows), production (109 rows)
```

**Impact:** Eliminates test-only code paths while maintaining strict production validation

---

## üìã **Specific PRD Updates Needed**

### **Update 1: STD-qa-testing-prd-v1.0.md**

**Section to Add:** ¬ß5.1.1 "Golden Fixture Hygiene"

```markdown
## ¬ß5.1.1 Golden Fixture Hygiene

### Clean Data Requirement
Golden fixtures must contain clean, idealized data:
- ‚úÖ No duplicate natural keys
- ‚úÖ No missing required values
- ‚úÖ No out-of-range values
- ‚úÖ No data quality issues

### Test Pattern
```python
# Golden test assertions (REQUIRED)
assert len(result.rejects) == 0, "No rejects expected for clean golden fixture"
assert len(result.data) == EXPECTED_COUNT, "Exact count for determinism"
```

### Real-World Quirks
Test authentic CMS data issues separately:
- Create `tests/fixtures/<dataset>/edge_cases/` directory
- Document real CMS quirks (duplicate codes, unusual patterns)
- Use `@pytest.mark.edge_case` marker
- Assert expected rejects: `assert len(result.rejects) > 0`

### Examples
See: `tests/ingestion/test_gpci_parser_golden.py` (hybrid approach)
- Golden tests: 18 clean rows, 0 rejects
- Edge case: Duplicate locality 00 (AL + AZ), 2 rejects
```

---

### **Update 2: STD-qa-testing-prd-v1.0.md**

**Section to Add:** ¬ß5.1.2 "Multi-Format Fixture Parity"

```markdown
## ¬ß5.1.2 Multi-Format Fixture Parity

### Identical Data Requirement
All format variations must contain identical data:
- TXT, CSV, XLSX, ZIP must have same rows
- Enables true format consistency tests
- Prevents fixture drift

### Creation Process
1. Create master fixture (e.g., TXT with 18 clean rows)
2. Derive other formats from master
3. Verify all formats parse to identical output
4. Document in fixture README

### Consistency Test Pattern
```python
# True format consistency (REQUIRED for multi-format parsers)
assert txt_result.data.equals(csv_result.data)
assert set(txt_result.data['key']) == set(csv_result.data['key'])

# NOT flexible assertions
assert len(overlapping) >= 10  # ‚ùå Hides fixture drift
```

### Exception: Full Dataset XLSX
If XLSX contains full dataset (100+ rows) for comprehensive testing:
- Document in README: "XLSX contains full dataset"
- Create separate test: `test_<parser>_golden_xlsx_full_dataset()`
- Don't compare against sample TXT/CSV
```

---

### **Update 3: STD-parser-contracts-prd-v1.0.md**

**Section to Add:** ¬ß21.3 "Tiered Validation Thresholds"

```markdown
## ¬ß21.3 Tiered Validation Thresholds

### Principle
Use severity tiers (INFO/WARN/ERROR) instead of binary pass/fail to support both test fixtures and production data.

### Standard Pattern
```python
def validate_<metric>(df: pd.DataFrame) -> Optional[str]:
    """
    Tiered validation with informational, warning, and error levels.
    
    Returns:
        - None: Value in expected range
        - Warning message: Value unusual but not critical
        - Raises ParseError: Value critically invalid
    """
    value = calculate_metric(df)
    
    # ERROR tier: Critical failures
    if value == 0:
        raise ParseError(f"Critical: {metric} is 0")
    
    # INFO tier: Expected for test data
    if test_range_low <= value < test_range_high:
        return f"INFO: Test fixture range ({value}). Production expects {prod_range}."
    
    # WARN tier: Production boundary
    if value < prod_min or value > prod_max:
        return f"WARN: {metric} {value} outside production range {prod_range}."
    
    # SUCCESS tier
    return None
```

### Benefits
- No test-only flags/modes needed
- Tests exercise exact production code path
- Clear severity guidance for operators
- Supports edge cases, test fixtures, AND production

### Prohibited Patterns
```python
# ‚ùå WRONG: Test-only bypass
if metadata.get('skip_validation'):
    return

# ‚ùå WRONG: Binary threshold
if count < 100:
    raise ParseError()  # Fails all test fixtures

# ‚úÖ CORRECT: Tiered with INFO/WARN/ERROR
if count == 0:
    raise ParseError()  # ERROR
elif count < 100:
    logger.warning(f"INFO: {count} rows")  # INFO, doesn't fail
```

### Implementation Checklist
- [ ] Row count validation uses tiers
- [ ] Range validation uses tiers  
- [ ] Categorical validation uses tiers
- [ ] No test-only flags in metadata
- [ ] Tests validate warnings are logged
```

---

### **Update 4: STD-qa-testing-prd-v1.0.md**

**Section to Enhance:** ¬ß2.2 "Negative Testing" ‚Üí Add pytest markers

```markdown
## ¬ß2.2.1 Test Categorization with Markers

### Required pytest Markers
```python
# pyproject.toml
[tool.pytest.ini_options]
markers = [
    "golden: marks tests as golden/happy path tests",
    "edge_case: marks tests for real-world data quirks",
    "negative: marks tests for error/invalid input cases",
    ...
]
```

### Usage Pattern
```python
@pytest.mark.golden
def test_parser_golden_txt():
    """Clean fixture, happy path, no rejects."""
    assert len(result.rejects) == 0

@pytest.mark.edge_case
def test_parser_real_cms_quirk():
    """Authentic CMS data issue, validates error handling."""
    assert len(result.rejects) > 0

@pytest.mark.negative
def test_parser_invalid_input():
    """Deliberately malformed data, expects ParseError."""
    with pytest.raises(ParseError):
        parse_data(bad_input)
```

### Benefits
- Clear test categorization
- Selective test running: `pytest -m golden`
- Proper separation of concerns
- Standards-aligned organization
```

---

### **Update 5: Add to all Parser PRD Templates**

**Section to Add:** "Testing Strategy & Fixtures"

```markdown
## Testing Strategy

### Golden Test Fixtures (Required)
**Location:** `tests/fixtures/<dataset>/golden/`

**Requirements:**
- Clean data (no duplicates, no quality issues)
- Identical across all formats (TXT/CSV/XLSX/ZIP)
- Deterministic row counts
- Zero expected rejects
- SHA-256 hashes documented

**Size Guidance:**
- Minimum: 10 unique rows (edge coverage)
- Recommended: 15-25 rows (good coverage without bloat)
- Maximum: 50 rows (unless full dataset needed)

### Edge Case Fixtures (Required if CMS has quirks)
**Location:** `tests/fixtures/<dataset>/edge_cases/`

**Purpose:** Test real CMS data quirks
- Duplicate codes (e.g., locality 00 for AL + AZ)
- Format variations across years
- Unusual but authentic patterns

**Pattern:**
- Create separate fixtures for each quirk
- Document CMS provenance
- Assert expected rejects/warnings
- Link to CMS documentation of quirk

### Test Organization
```
tests/
  fixtures/
    <dataset>/
      golden/          ‚Üê Clean data, rejects == 0
        README.md
        sample.txt
        sample.csv  
        sample.xlsx
        sample.zip
      edge_cases/      ‚Üê Real quirks, rejects > 0
        README.md
        duplicate_keys.txt
        unusual_format.csv
      negatives/       ‚Üê Invalid data, expect errors
        README.md
        out_of_range.csv
        missing_column.csv
  ingestion/
    test_<dataset>_parser_golden.py
    test_<dataset>_parser_edge_cases.py
    test_<dataset>_parser_negatives.py
```
```

---

## üéØ **Summary of All 15 Lessons**

### **Original 10 Lessons (Implementation Phase)**
1. Test fixtures must match real CMS data format
2. Alias maps need comprehensive CMS header variations  
3. Type handling must be defensive
4. Format detection should be content-based
5. Error messages should be actionable
6. Pre-implementation validation is critical
7. CMS data quirks need explicit documentation
8. Test coverage should include format-specific edge cases
9. Metrics calculation should handle missing/empty data
10. Incremental testing is more efficient than big-bang

### **New 5 Lessons (QTS Compliance Phase)**
11. **Golden fixtures must be clean, edge cases tested separately**
12. **Test-only flags/modes violate production parity**
13. **Exact assertions enable true regression detection**
14. **Format fixture parity is critical for consistency tests**
15. **Tiered validation thresholds support both test and production**

---

## ‚è±Ô∏è **Time Savings Impact**

**GPCI Parser Journey:**
- Implementation: 2 hours
- Initial testing (60% pass): 1 hour debugging
- Docker/environment setup: 2 hours
- Format detection fixes: 2 hours  
- QTS compliance: 1 hour
- **Total: ~8 hours**

**Future Parser with All Lessons Applied:**
- Implementation: 1.5 hours (phased, incremental)
- Initial testing (>80% pass): 15 min (clean fixtures)
- Environment: 10 min (already set up)
- Format detection: 15 min (pattern established)
- QTS compliance: 0 min (built-in from start)
- **Total: ~2.5 hours** 

**Time Savings: 70% (8h ‚Üí 2.5h)** üöÄ

---

## üìö **PRD Update Priority**

**Critical (P0) - Apply before next parser:**
1. Add ¬ß5.1.1 "Golden Fixture Hygiene" to QTS (Lesson #11)
2. Add ¬ß5.1.2 "Multi-Format Fixture Parity" to QTS (Lesson #14)
3. Add ¬ß21.3 "Tiered Validation Thresholds" to Parser Contracts (Lesson #12, #15)

**High (P1) - Apply this sprint:**
4. Add "Testing Strategy & Fixtures" section to parser PRD template (Lesson #11-15)
5. Enhance ¬ß2.2 with pytest markers in QTS (Lesson #11)
6. Add assertion precision requirements to QTS (Lesson #13)

---

**These 15 lessons, when codified in PRDs, will reduce future parser implementation + QTS compliance from ~8 hours to ~2.5 hours.**

